# Deep Deterministic Policy Gradient
Requirements: [[DQN]]

## Resources
- First DDPG paper: [Deterministic Policy Gradients (Silver et al. 2014)](http://proceedings.mlr.press/v32/silver14.pdf)
- Second DDPG paper: [Continuous Control with Deep Reinforcement Learning (Lillicrap et al. 2015)](https://arxiv.org/abs/1509.02971)

## Introduction
We've seen how [[DQN]] allows us to  extract $\pi^*(s)$ in discrete action spaces by learning $Q^*(s, a)$ and explicitly evaluating $\mathop{\text{argmax}}_{a\in A}Q^*(s, a)$. Could a similar strategy work in continuous spaces, even though we can't explicitly calculate $\pi^*$ given $Q^*$?

## Strategy
Store rollout data in a replay buffer $\mathcal{D}$ and create 
- a policy network $\pi_\phi(s)$
- a value network $Q_\theta(s, a)$
- a target policy network $\pi_{\phi'}(s)$, where  $\phi'$ is a moving average of $\phi$
- a target value network $Q_{\theta'}(s, a)$, where $\theta'$ is a moving average of $\theta$

(We can implement a moving average by performing the following each step: $\theta' \leftarrow \rho \theta' + (1-\rho)\theta$, with $\rho$ slightly less than 1.)

Parameterize a deterministic policy $\pi: S\to A$ and then use a neural net to learn parameters $\phi$ of the policy $\pi^*$ which maximizes $\mathop{\mathbb{E}}_{s\sim \mathcal{D}}Q_\theta(s, \pi_\phi(s))$. 

Note this uses an approximation of $Q^*$ instead of the policy gradient theorem for policy iteration, so we don't need to guarantee $s\sim\mu^{\pi}(s\mid s_0)$, meaning DDPG is **off-policy**. This allows us to utilize the full replay buffer when training our policy.

As in [[DQN]], use $\mathcal{D}$ to retrieve data for training $Q_\theta$, and use $Q_{\theta'}$ to construct the targets as usual. Since $\pi_\phi$ is trained to maximize $Q_\theta$, approximate $\max_{a\in A}$ in the [[Value Functions#^5e3551|Bellman Recurrence]] via the (more stable) target policy network $\pi_{\phi'}$:
$$\max_{a\in A} Q_{\theta'}(s, a) \approx Q_{\theta'}(s, \pi_{\phi'}(s))$$

Lastly, to encourage exploration, add zero-mean Gaussian noise to actions generated by the policy. ^d23e5c

## Use Cases
DDPG is designed to be applicable broadly. It is applicable to
- Discrete and Continuous State Spaces
- Discrete and Continuous Action Spaces

## Algorithm
**DDPG**: Deep Deterministic Policy Gradient
**Parameters**: 
- Number training steps $T$
- Replay buffer capacity $N$
- Noise parameter $\varepsilon$
- Learning rates for gradient descent
- Target update rate $\rho$

**Algorithm**:
> Initialize replay buffer $\mathcal{D}$ with capacity $N$\
> Initialize policy NN $\pi_\phi$ and value NN $Q^*_\theta$
> **for** step $\in\{1, \dots, T\}$ **do**:\
> $\qquad$ Retrieve $a= \pi_\phi(s) + \delta$, where $\delta\sim\mathcal{N}(0, \varepsilon)$\
> $\qquad$ Execute action $a$, retrieve reward $r$ and next state $s'$\
> $\qquad$ Sample transitions $(s_j, a_j, r_j, s_{j+1}, d_j)$ from $\mathcal{D}$\
> $\qquad$ Update $\phi$:
> $\qquad\qquad$ Set $L(\phi) = -\sum_jQ_{\theta}(s_j, \pi_\phi(s_j))$
> $\qquad\qquad$ Perofrm GD on $L$ to update $\phi$
> $\qquad$ Update $\theta$:
> $\qquad\qquad$ Set $y_j = r_j +\gamma (1 - d_j)Q_{\theta'}(s_{j+1}, \pi_{\phi'}(s_{j+1}))$\
> $\qquad\qquad$ Set $L(\theta) = \sum_{j} \left(y_j - Q_\theta(s_j,a_j )\right)^2$\
> $\qquad\qquad$ Perform GD on $L$ to update $\theta$
> $\qquad$ Update $\theta'$ and $\phi'$:
> $\qquad \qquad$ $\theta'\leftarrow \rho \theta' + (1-\rho)\theta$
> $\qquad \qquad$ $\phi'\leftarrow \rho \phi' + (1-\rho)\phi$




## Miscellaneous
- Here we sample train-time action noise from a Gaussian, but the original DDPG paper used the time-correlated Ornstein-Uhlenbeck process. The action noise is indexed by trajectory time $t$, and follows $$\delta_{t+1} = (1-\theta)\delta_t + \varepsilon \delta'\text{,  where  } \delta'\sim\mathcal{N}(0, 1)$$
- DDPG is typically applied to bounded, continuous action spaces, in which case we clip the actions $a = \pi_\phi(s) + \delta$ to be within the action space bounds.