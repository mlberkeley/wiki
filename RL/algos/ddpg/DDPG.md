# Deep Deterministic Policy Gradient
Requirements: [[DQN]]

## Resources
- [Deterministic Policy Gradients (Silver et al. 2014)](http://proceedings.mlr.press/v32/silver14.pdf)
- [Continuous Control with Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971)

## Introduction
We've seen how [[DQN]] allows us to  extract $\pi^*(s)$ in discrete action spaces by learning $Q^*(s, a)$ and explicitly evaluating $\argmax_{a\in A}Q^*(s, a)$. Could a similar strategy work in continuous spaces, even though we can't explicitly calculate $\pi^*$ given $Q^*$?

## Strategy
Store rollout data in a replay buffer $\mathcal{D}$ and create 
- a policy network $\pi_\theta(s)$
- a value network $Q_\phi(s, a)$
- a target policy network $\pi_{\theta'}(s)$, where  $\theta'$ is a moving average of $\theta$
- a target value network $Q_{\phi'}(s, a)$, where $\phi'$ is a moving average of $\phi$

(We can implement a moving average by performing the following each step: $\theta' \leftarrow \rho \theta' + (1-\rho)\theta$, with $\rho$ slightly less than 1.)

Parameterize a deterministic policy $\pi: S\to A$ and then use a neural net to learn parameters $\theta$ of the policy $\pi^*$ which maximizes $\E_{s\sim S}Q_\phi(s, \pi_\theta(s))$. 

As in [[DQN]], use $\mathcal{D}$ to retrieve data for training $Q_\phi$, and use $Q_{\phi'}$ to construct the targets as usual. Approximate $\max_{a\in A}$ in the [[Value Functions#^5e3551|Bellman Recurrence]] via the target policy network $\pi_{\theta'}$, which would be trained to maximize the target value network $Q_{\phi'}$ given the policy's training objective:
$$\max_{a\in A} Q_{\phi'}(s, a) \approx Q_{\phi'}(s, \pi_{\theta'}(s))$$

Lastly, to encourage exploration, add zero-mean Gaussian noise to actions generated by the policy.

## Use Cases
DDPG is designed to be applicable broadly. It is applicable to
- Discrete and Continuous State Spaces
- Discrete and Continuous Action Spaces

## Algorithm
**DDPG**: Deep Deterministic Policy Gradient
**Parameters**: 
- Number training steps $T$
- Replay buffer capacity $N$
- $Q$-value estimator (see [[PG#^df0b0f|above]])

**Algorithm**:
> Initialize replay buffer $\mathcal{D}$ with capacity $N$\
> Initialize policy NN $\pi_\theta$ and value NN $Q^*_\phi$
> **for** step $\in\{1, \dots, T\}$ **do**:\
> $\qquad$ Retrieve $a= \pi_\theta(s) + \delta$, where $\delta\sim\mathcal{N}(0, \varepsilon)$\
> $\qquad$ Execute action $a$, retrieve reward $r$ and next state $s'$\
> $\qquad$ Update $\theta$:
> $\qquad\qquad$ Set $L(\theta) = Q_{\phi}(s, \pi_\theta(s) + \delta)$
> $\qquad\qquad$ Perofrm GD on $L$ to update $\theta$
> $\qquad$ Update $\phi$:
> $\qquad\qquad$ Sample $N$ actions $\{a_i\}_{i\in[N]}$ uniformly from $A$
> $\qquad\qquad$ Set $L(\phi) = \left[Q_\phi(s, a) - r - \gamma Q_{\phi'}(s', \pi_{\theta'}(s))\right]^2$
> $\qquad\qquad$ Perform GD on $L$ to update $\phi$
> $\qquad$ Update $\theta'$ and $\phi'$:
> $\qquad \qquad$ $\theta'\leftarrow \rho \theta' + (1-\rho)\theta$
> $\qquad \qquad$ $\phi'\leftarrow \rho \phi' + (1-\rho)\phi$

## Miscellaneous
- Here we sample train-time action noise from a Gaussian, but the original DDPG paper used the time-correlated Ornstein-Uhlenbeck process. The action noise is indexed by trajectory time $t$, and follows $$\delta_{t+1} = (1-\theta)\delta_t + \varepsilon \delta'\text{,  where  } \delta'\sim\mathcal{N}(0, 1)$$