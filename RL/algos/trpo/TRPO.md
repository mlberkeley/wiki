# Trust Region Policy Optimization
Requirements: [[Importance Sampling for PG]], [[Natural PG]], [[PG]], [[Conjugate Gradient Method]], [[KL Divergence]], [[Pinsker's Inequality]]
## Resources
- [Spinning Up](https://spinningup.openai.com/en/latest/algorithms/trpo.html)
- [Original TRPO Paper](https://arxiv.org/abs/1502.05477)
## Introduction
The traditional on-policy [[PG|Policy Gradient]] requires that we train on trajectories generated by the current policy $\pi_\theta$. In practice, however, we would like to be able to train **off-policy**, meaning on trajectories gathered from a prior policy $\pi_{\phi}$. We've explored [[Importance Sampling for PG|importance sampling]] as a strategy to accomplish this, and we can also apply it to the [[Advantage Function]] version of the policy gradient:
$$\begin{align*}\nabla_{\theta}J(\theta) &\propto \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\theta}(s\mid s_0)}\; 
\mathop{\mathbb{E}}_{a\sim\pi_\theta(s)}\Big[A^{\pi_\theta}(s, a)\nabla_{\theta} \ln\pi_\theta(a\mid s)\Big]\\&\propto \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_\phi(s)}\left[\frac{\mu^{\pi_\theta}(s\mid s_0)\pi_\theta(a\mid s)}{\mu^{\pi_\phi}(s\mid s_0)\pi_\phi(a\mid s)}A^{\pi_\theta}(s, a)\nabla_{\theta} \ln\pi_\theta(a\mid s)\right]\end{align*}$$
Previously, we've employed high-variance estimates for the importance weight, and calculated the gradients above by optimizing a surrogate objective which treated the importance weight and Advantage estimates as constant with respect to $\theta$.

What if we instead constrained  $\pi_\theta$ to be close to $\pi_\phi$ so that the numerator and denominator are similar enough to be ignored? It turns out that, for discrete state and action spaces and deterministic dynamics,
$$\begin{align*}D_{\text{KL}}(\pi_\phi(\cdot\mid s) \vert\vert \pi_\theta(\cdot\mid s)) \leq \varepsilon &\implies \lvert \pi_\theta(a\mid s)-\pi_\phi(a\mid s)\rvert\leq \sqrt{\frac{\varepsilon}{2}}\\&\implies \lvert \mu^{\pi_\theta}(s_t\mid s_0)-\mu^{\pi_\phi}(s_t\mid s_0)\rvert \leq t\sqrt{2\varepsilon}\end{align*}$$
This implies that we can keep the policies and state visitation distributions similar (and hence the importance weights close to 1) by bounding the KL Divergence between the policies: 
$$D(\pi_\phi\vert\vert\pi_\theta) = \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\sim s_0)}D_{\text{KL}}(\pi_\phi(\cdot\mid s) \vert\vert \pi_\theta(\cdot\mid s)) \leq \varepsilon$$
which intuitively makes sense in contniuous state and action spaces with stochastic dynamics as well.

We call the space of policies $\pi_\theta$ that are close to $\pi_\phi$ the **trust region**, since we know the state visitation distributions are similar enough here that we can generally ignore their contribution to the importance weights. This allows us to greatly simplify the importance-weighted policy gradient (we use the baselined)

$$\begin{align*}\nabla_{\theta}J(\theta) &\propto \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_\phi(s)}\left[\frac{\pi_\theta(a\mid s)}{\pi_\phi(a\mid s)}A^{\pi_\theta}(s, a)\nabla_{\theta} \ln\pi_\theta(a\mid s)\right]\\&\propto \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_\phi(s)}\left[\frac{\pi_\theta(a\mid s)}{\pi_\phi(a\mid s)}A^{\pi_\theta}(s, a) \frac{\nabla_{\theta}\pi_\theta(a\mid s)}{\pi_\theta(a\mid s)}\right]\\&\propto \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_\phi(s)}\left[\frac{\nabla_{\theta}\pi_\theta(a\mid s)}{\pi_\phi(a\mid s)}A^{\pi_\theta}(s, a)\right]\\&\propto \nabla_{\theta}\mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_\phi(s)}\left[\frac{\pi_\theta(a\mid s)}{\pi_\phi(a\mid s)}A^{\pi_\theta}(s, a)\right]\end{align*}$$
If we again treat $A^{\pi}(s, a)$ as constant with respect to $\theta$, we can simply optimize the surrogate objective:
$$J(\theta) = \mathop{\mathbb{E}}_{s\sim\mu^{\pi_\phi}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_\phi(s)}\left[\frac{\pi_\theta(a\mid s)}{\pi_\phi(a\mid s)}A^{\pi}(s, a)\right]$$

We can take gradient steps subject to the KL constraint according to the [[Natural PG|Natural Policy Gradient]], which uses a quadratic approximation for the Hessian of the KL constraint. Since this approximation can be erroneous, we perform backtracking along the gradient direction until $D(\pi_{\theta_t}\vert\vert\pi_{\theta_{t+1}})\leq\varepsilon$ as desired.

## Strategy
Learn a value function $V^\pi_\phi$ for advantage estimation. Update the policy parameters $\theta$ using the [[Natural PG|Natural Policy Gradient]] applied to the following surrogate objective:
$$\begin{align*}J(\theta) &= \mathop{\mathbb{E}}_{s\sim\mu^{\pi_{\theta_k}}(s\mid s_0)}
\mathop{\mathbb{E}}_{a\sim\pi_{\theta_k}(s)}\left[\frac{\pi_\theta(a\mid s)}{\pi_{\theta_k}(a\mid s)}A^{\pi_{\theta_k}}(s, a) \right]\\&\approx \frac{1}{NT}\sum_{i=1}^N\sum_{t=1}^T \frac{\pi_\theta\left(a_t^{(i)}\Big\vert s_t^{(i)}\right)}{\pi_{\theta_k}\left(a_t^{(i)}\Big\vert s_t^{(i)}\right)}\hat A^{(i)}_t\end{align*}$$
$$\theta_{t+1} \leftarrow \theta_t - \varepsilon \alpha^j\frac{H^{-1}\nabla_\theta J(\theta)}{\sqrt{\frac{1}{2}\nabla_\theta J(\theta)^TH^{-1}\nabla_\theta J(\theta)}}, \qquad j = \min\{j\mid D(\pi_{\theta_t}\vert\vert\pi_{\theta_{t+1}}) \leq \varepsilon\}$$

where $\hat A^{(i)}_t$ is an [[Advantage Function#Advantage Estimation|advantage estimate]], $\varepsilon$ is the KL limit, $\alpha < 1$ is a backtracking parameter, and $H$ is the Hessian of the KL constraint. Use the following formula to estimate $Hx$ using samples for any $x$, including $\nabla_\theta J(\theta)$ in the denominator:
$$\begin{align*}Hx&=\mathop{\mathbb{E}}_{s\sim\mu^{\pi_{\theta_t}}(s\mid s_0)}\mathop{\mathbb{E}}_{a\sim\pi_{\theta_t}(s)}\nabla_\theta\left[\left(-\nabla_\theta\log\pi_\theta(a\mid s)\right)^Tx\right]\\&\approx \frac{1}{NT}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\left[\left(-\nabla_\theta\log\pi_\theta\left(a_t^{(i)}\mid s_t^{(i)}\right)\right)^Tx\right]\end{align*}$$
Use the above formula and the [[Conjugate Gradient Method]] to find $H^{-1}\nabla_\theta J(\theta)$, and then we can simply calculate and apply the weight update.

Each gradient step, we start with $j=1$, find $\theta_{t+1}$, and estimate the KL constraint using samples to check that it is satisfied: 
$$\mathop{\mathbb{E}}_{s\sim\mu^{\pi_{\theta_t}}(s\sim s_0)}D_{\text{KL}}(\pi_{\theta_t}(\cdot\mid s) \vert\vert \pi_{\theta_{t+1}}(\cdot\mid s))\approx \frac{1}{NT}\sum_{i=1}^N\sum_{t=1}^T\ln\frac{\pi^{(i)}_t}{\pi_{\theta_{t+1}}\left(a^{(i)}_t\mid s^{(i)}_t\right)} \leq \varepsilon$$
If not, we iterate $j$, find $\theta_{t+1}$ for this new value of $j$, and then again estimate the KL constraint to check satisfaction. This backtracking ensures our policy iteration remains with the trust region defined by $D_{\text{KL}}(\pi_{\theta_t}\vert\vert\pi_\theta)$.

## Use Cases
TRPO is applicable to

-   Discrete and Continuous State Spaces
-   Discrete and Continuous Action Spaces

## Algorithm

**TRPO**: Trust Region Policy Optimization
**Parameters**: 
- Maximum episode length $T$
- Number of episodes per iteration $N$
- Number of iterations $K$
- Backtracking coefficient $\alpha$
- KL Divergence Limit $\varepsilon$
- [[Advantage Function#Estimating Advantage|Advantage Function Estimator]] $\mathop{\text{AdvEst}}(s,  r, V^\pi)$

**Algorithm**:
> Initialize policy NN $\pi_\theta$ and value NN $V^\pi_\phi$
> **for** epoch $k\in\{1, \dots, K\}$ **do**:\
> $\qquad$ **for** episode $i\in\{1, \dots, N\}$ **do**:\
> $\qquad\qquad$ **for** step $t\in\{1, \dots, T\}$ **do**:\
> $\qquad\qquad\qquad$ Take action $a_t^{(i)}= \pi_{\theta_k}\left(s^{(i)}\right)$ \
> $\qquad\qquad\qquad$ Get reward $r_t^{(i)}$, state $s_{t+1}^{(i)}$\
> $\qquad\qquad\qquad$ Store logprob $\pi_t^{(i)} = \pi_{\theta_k}\left(a_t^{(i)}\bigg\vert s_t^{(i)}\right)$\
> $\qquad$ Estimate advantage $A^{(i)}_t = \mathop{\text{AdvEst}}\left(s_t^{(i)},  r_t^{(i)}, V^\pi_\phi\right)$ \
> $\qquad$ Calculate $\hat g = \nabla_\theta\left[\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^TA_t^{(i)}\frac{\pi_\theta\left(a_t^{(i)}\mid s_t^{(i)}\right)}{\pi_t^{(i)}}\right]$ \
> $\qquad$ Use $Hx= -\nabla_\theta\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^T\nabla_\theta\ln\pi_\theta\left(a_t^{(i)}\vert s_t^{(i)}\right)^Tx$ \
> $\qquad$ Calculate $\hat x = H^{-1}\hat g$ using the [[Conjugate Gradient Method|CGM]] and $Hx$ \
> $\qquad$ **for** $j\in\{0, 1, \dots\}$ **do**: \
> $\qquad\qquad$ $\theta_{k+1}\leftarrow\theta_k +\alpha^j \sqrt{\frac{2}{\hat g^T\hat x}}\hat x$ \
> $\qquad \qquad$ **if** $\frac{1}{NT}\sum_{i=1}^N\sum_{t=1}^T\ln\frac{\pi_t^{(i)}}{\pi_{\theta_{k+1}}\left(a_t^{(i)}\mid s_t^{(i)}\right)} \leq \varepsilon$: \
> $\qquad\qquad\qquad$ **break** \
> $\qquad$ $L(\phi) = \frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \left(r_t^{(i)} + \gamma V^\pi_\phi\left(s_{t+1}^{(i)}\right)-V^\pi_\phi\left(s_t^{(i)}\right)\right)$ \
> $\qquad$ Run GD on $L(\phi)$ to update $\phi$

## Analysis
Here we prove that constraining expected [[KL Divergence]] between policies effectively constrains the distance between their policy distributions and state visitiation distributions for discrete state and action spaces and deterministic dynamics:
$$\begin{align*}D_{\text{KL}}(\pi_\phi(\cdot\mid s) \vert\vert \pi_\theta(\cdot\mid s)) \leq \varepsilon &\implies \lvert \pi_\theta(a\mid s)-\pi_\phi(a\mid s)\rvert\leq \sqrt{\frac{\varepsilon}{2}}\\&\implies \lvert \mu^{\pi_\theta}(s_t\mid s_0)-\mu^{\pi_\phi}(s_t\mid s_0)\rvert \leq t\sqrt{2\varepsilon}\end{align*}$$
The first of these facts follows directly from [[Pinsker's Inequality]], which states that for any distributions $P$ and $Q$ on a space $X$:
$$\sup_{x\in X}\big\lvert P(x) - Q(x)\big\rvert \leq \sqrt{\frac{D_{\text{KL}}(P\vert\vert Q)}{2}}$$
We can define a joint distribution over actions taken by 
$\pi_\theta$ and $\pi_\phi$ in a state $s_t$, as well as another joint distribution over the states $\pi_\theta$ and $\pi_\phi$ end up in:
$$p(a_t, a_t'\mid s_t) = \pi_\theta(a_t\mid s_t)\pi_\phi(a_t'\mid s_t)$$
$$p(s_t, s_t'\mid s_0) = \mu^{\pi_\theta}(s_t\mid s_0)\mu^{\pi_\phi}(s_t'\mid s_0)$$
Under these conditions, given we are in a finite, discrete action space, we have the following bound on the probability that $\pi_\theta$ and $\pi_\phi$ take the same actions at any timestep: $$\lvert \pi_\theta(a\mid s)-\pi_\phi(a\mid s)\rvert\leq \sqrt{\frac{\varepsilon}{2}}\implies p(a_t=a_t'\mid s_t) \geq 1 - \sqrt{\frac{\varepsilon}{2}}$$

If $\pi_\theta$ and $\pi_\phi$ have probability at least $1 - \sqrt{\frac{\varepsilon}{2}}$ of taking the same action each step, then they have probability at least $\left(1 - \sqrt{\frac{\varepsilon}{2}}\right)^t\geq 1 - t\sqrt{\frac{\varepsilon}{2}}$ of taking the same actions $a_0, \dots, a_{t-1}$ in all $t$ steps leading up to $s_t$. 

Since the dynamics are deterministic, $\pi_\theta$ and $\pi_\phi$ end up in the same state $s_t$ when they take all the same actions leading up to timestep $t$. This allows us to bound the difference between the state visitation distributions by bounding the joint probability that we land in the same state:
$$p(s_t=s_t'\mid s_0) \geq 1 - t\sqrt{\frac{\varepsilon}{2}}\implies \lvert \mu^{\pi_\theta}(s_t\mid s_0)-\mu^{\pi_\phi}(s_t\mid s_0)\rvert\leq t\sqrt{\frac{\varepsilon}{2}}$$
