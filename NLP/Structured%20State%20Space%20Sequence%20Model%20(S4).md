---
bibliography: ""
---

# Structured State Space Sequence Model (S4)

**Transformers**. "Transformers are one of the most celebrated models in machine learning, providing breakthrough performance on many language and vision tasks. However, the quadratic time complexity in computing attention makes transformers unsuitable for sequence data with very long-range dependencies. Various attempts have been made to modify the attention mechanism to reduce this complexity, with various degrees of success.
Now the era of the transformer is OVER, as it’s all about state space models! They’re a completely different alternative to learning dependencies in time-series data based on classical signal processing and control theory, as well reviving some old ideas used in RNNs and LSTMs. They COMPLETELY OBLITERATE transformers on long-context tasks such as Long Range Arena and seem like a promising candidate for future sequence tasks."

# Analysis

[Structured State Space Sequence Model (S4) Reading Group](link to recording)

* RG on 10/13/2022
* Tyler and Sohum
* Slides: https://docs.google.com/presentation/d/1nmAm1zUV4mrUTyzQXubx8Cb1bg8pziPcq5CHzfiPGNs/edit


# Synthesis

Think of this section as expert-level. The reader knows the high-level idea as well as the fundamental building blocks. Synthesize (put it together) for them. Don't be afraid to present a complicated proof if you deem it relevant.

---