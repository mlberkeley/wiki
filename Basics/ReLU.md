**Rectified linear unit (ReLU)** is an activation function

$$
\mathsf{ReLU}(x) = \max(0, x)
$$