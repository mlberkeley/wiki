The **Kullback-Leibler (KL) divergence**, also known as **relative entropy**, $D_{KL}(P \parallel Q)$ tells us how inefficient (number of bits wasted) it is to use distribution $Q$ to model messages that come from distribution $P$.

$$
D_{KL}(P \parallel Q) = H_Q[X] - H[X]
$$